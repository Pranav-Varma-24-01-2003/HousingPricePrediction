{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pranav-Varma-24-01-2003/Movie-Review-Sentiment-Classification-/blob/main/IMDB%20Sentiment%20Classification%20using%20TF-IDF/sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7dDrmfqw7C8"
      },
      "source": [
        "### IMDB Sentiment Classification using TF-IDF\n",
        "**Note:** The dataset used here is obtained from initial preprocessing on the [original dataset](http://ai.stanford.edu/~amaas/data/sentiment/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMes9CR_whPo",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.simplefilter('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWIIOXlfwhPt",
        "outputId": "b0b6a99f-ab2c-4254-99b5-dc9c4f018fa2",
        "scrolled": false
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pylab as plt\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pickle\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "import os\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUOOSSK4whP0"
      },
      "source": [
        "#### Import and prepare dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gg3evPY5whP2",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('./datasets/data.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27Bb0UAFzqgX"
      },
      "source": [
        "Let's see what's inside the *data*!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "skxKwp24whP5",
        "outputId": "1cb014b3-c199-4200-d5d1-7c91d0268e30",
        "scrolled": false
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>this was painful i made myself watch it until ...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>once again mr costner has dragged out a movie ...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>by strange coincidence i ve started to watch t...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>well the hero and the terror is slightly below...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>well the hero and the terror is slightly below...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review  label\n",
              "0  this was painful i made myself watch it until ...    0.0\n",
              "1  once again mr costner has dragged out a movie ...    0.0\n",
              "2  by strange coincidence i ve started to watch t...    0.0\n",
              "3  well the hero and the terror is slightly below...    0.0\n",
              "4  well the hero and the terror is slightly below...    0.0"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOcuGkk70Fma"
      },
      "source": [
        "Here, Label $0.0$ means **_negative_** *sentiment*, while $1.0$ means **_positive_** *sentiment*.\n",
        "\n",
        "Now, let's check if our dataset contains any null values, if yes, drop those rows!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "AaMg5f6xwhP-",
        "outputId": "c4aac5d5-5645-4b00-f95a-3e936677eddf",
        "scrolled": false
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "review    2\n",
              "label     5\n",
              "dtype: int64"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxFwqZG1whQD",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "data = data.dropna().reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "XzD0qFeMwhQH",
        "outputId": "6d24997c-d5cc-4282-a719-4d4e0c078062"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "review    0\n",
              "label     0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEuLHvSy1PgV"
      },
      "source": [
        "Now it looks good!\n",
        "\n",
        "Next step, split the dataset in train-test!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkNJjcEToRHR"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(data['review'], data['label'], test_size = .4, shuffle = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "3E-11-MboRHV",
        "outputId": "6b528fa3-1031-4f9e-ab97-7c5e6c4c615f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(29446, 29446, 19632, 19632)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(X_train), len(y_train), len(X_test), len(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noJx6s-9oRHZ"
      },
      "source": [
        "#### A little about number of sentiments!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "BEyqdxI6oRHa",
        "outputId": "8da8b68c-faf9-49ed-f44c-8b512bd4091e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentiments in entire dataset:\n",
            " Positive: 24536\n",
            " Negative: 24542\n"
          ]
        }
      ],
      "source": [
        "sentiments = data['label'].value_counts()\n",
        "print('Sentiments in entire dataset:\\n Positive: {}\\n Negative: {}'.format(sentiments[1], sentiments[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOHOoZ03oRHe"
      },
      "source": [
        "After split,\n",
        "- Number of **_positive sentiments_** in **_train data + test data_** should be equal to the **_total number positive sentiments_** in entire dataset,\n",
        "- Likewise, Number of **_neagetive sentiments_** in **_train data + test data_** should be equal to the **_total number negative sentiments_** in entire dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQUqHzGeoRHf"
      },
      "outputs": [],
      "source": [
        "def get_sentiments(d, _d):\n",
        "    positive = (d==1).sum()\n",
        "    negative = (d==0).sum()\n",
        "    print('Sentiments in {}:\\n Positive: {}\\n Negative: {}'.format(_d, positive, negative))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "id": "N-xS-FT4oRHk",
        "outputId": "20b13705-51f2-4ead-a0c0-25e3a4ec1e25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentiments in Train data:\n",
            " Positive: 12333\n",
            " Negative: 17113\n",
            "Sentiments in Test data:\n",
            " Positive: 12203\n",
            " Negative: 7429\n"
          ]
        }
      ],
      "source": [
        "get_sentiments(y_train, 'Train data')\n",
        "get_sentiments(y_test, 'Test data')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGuzQZQg17B7"
      },
      "source": [
        "Everything seems right! Let's proceed further!"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cO9dvYh-Firo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLAoIbqWwhQd"
      },
      "source": [
        "#### TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vzy2Hsr6J8y"
      },
      "source": [
        "Now, as we all know, we cannot feed the data to a classifier as it is. We should first convert these data into a numerical form known as vectors.\n",
        "\n",
        "Here, we will use **TF-IDF** (**T**erm **F**requency–**I**nverse **D**ocument **F**requency), a numerical statistic that reflects how important a word is to a document in a collection or corpus by assigning some weight to it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jidIRxpwhQf"
      },
      "outputs": [],
      "source": [
        "def tokenize(text):\n",
        "    return [word for word in word_tokenize(text.lower()) if word not in stopwords.words('english')]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhB9NBOB5eMs"
      },
      "source": [
        "Since our dataset contains in total $49078$ reviews, generating vectors will take a lot of time. Doing so every time will be a time-consuming task.\n",
        "\n",
        "Therefore, once we generate vectors in the first run, we can _store_ the **vocabulary** in a separate file. We can then use this saved vocabulary to _fit_ our _train_ and _test data_ in future runs (*of this program, of course!*).\n",
        "\n",
        "The following method will **_initialize vectorizer_** based on the chosen option."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9WroF6gS-hE"
      },
      "outputs": [],
      "source": [
        "def choose_vectorizer(option):\n",
        "    if option == 'generate':\n",
        "        vectorizer = TfidfVectorizer(tokenizer = tokenize)\n",
        "    elif option == 'load':\n",
        "        vectorizer = TfidfVectorizer(vocabulary = pickle.load(open('vocabulary.pkl', 'rb')))\n",
        "\n",
        "    return vectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eerky24vS-hK"
      },
      "source": [
        "In following code cell, choose the option **_generate_** if you want to generate vectors of train-test data again (*this will also store the vocabulary in the same directory as this project*) otherwise go with the option **_load_** if you already have vocabulary (*saved in the same directory as this project*)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "JXo6F0s3oRH2",
        "outputId": "ce7646d7-c3ac-4c8f-d62f-c84fc018b8d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wall time: 35.5 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "options = ['generate', 'load']\n",
        "\n",
        "# 0 to generate, 1 to load (choose wisely, your life depends on it!)\n",
        "option = options[1]\n",
        "\n",
        "vectorizer = choose_vectorizer(option)\n",
        "vectorized_train_data = vectorizer.fit_transform(X_train)\n",
        "vectorized_test_data = vectorizer.transform(X_test)\n",
        "\n",
        "if option == 'generate':\n",
        "    pickle.dump(vectorizer.vocabulary_, open('vocabulary.pkl', 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_PmjPfYwhR0"
      },
      "source": [
        "#### Training and Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqGOoAZ97fwl"
      },
      "source": [
        "In the training dataset, there are more negative reviews than positive ones. Therefore, we will first make both the sides equal using **SMOTE** (**S**ynthetic **M**inority **O**ver-sampling **TE**chnique)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "zhPpi4lWwhRo",
        "outputId": "09386cdf-b4cc-4a55-aa9d-0ea8b0c13948"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wall time: 39.6 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "sm = SMOTE(random_state=42, ratio=1.0)\n",
        "X_train, y_train = sm.fit_sample(vectorized_train_data, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqcYdZLG8nn7"
      },
      "source": [
        "We will use **Logistic Regression** classifier here. So, let's train this guy!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msWBN71rwhR1"
      },
      "outputs": [],
      "source": [
        "clf = LogisticRegression()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "id": "T9KCDc-UoRIN",
        "outputId": "9e2c527d-6f7c-4180-8e8a-3915ac55298f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wall time: 4.38 s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcBSA9Pj8z-E"
      },
      "source": [
        "Of course, we will also do some **_cross-validation_**!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "2_ttCUJFoRIU",
        "outputId": "14c123ef-9bbe-4284-bd6f-4d896a1d10a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wall time: 37.5 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "kf = KFold(n_splits=10, random_state = 42, shuffle = True)\n",
        "scores = cross_val_score(clf, X_train, y_train, cv = kf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "l0eAWW06oRIX",
        "outputId": "cf56739d-abd9-44e3-f05b-85082c0bbcb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross-validation scores: [0.90972831 0.90972831 0.91206544 0.91235758 0.90359334 0.90914403\n",
            " 0.90298071 0.92226768 0.91320865 0.9181765 ]\n",
            "Cross-validation accuracy: 0.9113 (+/- 0.0112)\n"
          ]
        }
      ],
      "source": [
        "print('Cross-validation scores:', scores)\n",
        "print('Cross-validation accuracy: {:.4f} (+/- {:.4f})'.format(scores.mean(), scores.std() * 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGPqfmOX9XlD"
      },
      "source": [
        "The classifier is trained, so let's check its performance on the **_validation set_**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKG_jKupoRIa"
      },
      "outputs": [],
      "source": [
        "predictions = clf.predict(vectorized_test_data)\n",
        "\n",
        "validation = dict()\n",
        "\n",
        "validation['accuracy'] = accuracy_score(y_test, predictions)\n",
        "validation['precision'] = precision_score(y_test, predictions, average='macro')\n",
        "validation['recall'] = recall_score(y_test, predictions, average='macro')\n",
        "validation['f1'] = f1_score(y_test, predictions, average='macro')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "id": "xF9tWh-5whSF",
        "outputId": "7fb9be11-d388-485e-8d49-111b14840454",
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation results:\n",
            " ------------\n",
            "Accuracy: 0.86980\n",
            "Precision: 0.85957\n",
            "Recall: 0.87255\n",
            "F1: 0.86433\n"
          ]
        }
      ],
      "source": [
        "print('Validation results:\\n', '-' * 12)\n",
        "for v in validation:\n",
        "    print('{}: {:.5f}'.format(v.title(), validation[v]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYsPaCPkwhSg"
      },
      "source": [
        "#### Cohen-Kappa score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0BEQokS9ncF"
      },
      "source": [
        "**Cohen’s kappa** statistic measures **_interrater reliability_** which is more robust than a simple percent agreement calculation.\n",
        "\n",
        "In simple words, it will show us a level of agreement between the labels predicted by the classifier and the actual labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "fhSo9voTwhSj",
        "outputId": "8a1b2fed-6094-40db-8141-1164a44bc449"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "C-K Score: 0.72916\n"
          ]
        }
      ],
      "source": [
        "p = predictions.tolist()\n",
        "ck = cohen_kappa_score(y_test, p)\n",
        "print('C-K Score: {:.5f}'.format(ck))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzM2r3lU_q5R"
      },
      "source": [
        "#### It's time for a tiny test!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqGeds-D7E5N"
      },
      "outputs": [],
      "source": [
        "example_reviews = [\n",
        "    'An honest, engaging, and surprisingly funny look back at one of modern television\\'s greatest achievements.',\n",
        "    'Excellent movie! Inspiring and very entertaining for all especially youth and anyone inspired by today\\'s modern age of tech entrepreneurship!',\n",
        "    'Honestly even the trailer made me uncomfortable.',\n",
        "    'I never write movie reviews, but this one was such a stinker, I feel I owe it to everyone to at least provide a warning.',\n",
        "    'This movie was a good movie by standard and a lil beyond standard. It was written very well, The acting was great, each characters performance was clever and the comedic timing was spot on. The story line is very real and relatable. Enjoyable for adults and completely appropriate for pre-teens up to 20. Go support, my family loved it.'\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPbaqwo6xzeO"
      },
      "source": [
        "As one can tell, **first**, **second** and **fifth** reviews in **example_reviews** are **positive** while **third** and **fourth** reviews are **negative**. _Let's see what classifier predicts!_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "7OiDa5SkoRIr",
        "outputId": "b4453633-8fc4-486d-c473-117ba35da3fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 1 0 0 1\n"
          ]
        }
      ],
      "source": [
        "example_preds = clf.predict(vectorizer.transform(example_reviews))\n",
        "print(' '.join(str(int(p)) for p in example_preds))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "source_path = r\"'./datasets/data.csv'\"\n",
        "dest_path = r\"mnis_train.csv\"\n",
        "\n",
        "shutil.copyfile(source_path,dest_path)"
      ],
      "metadata": {
        "id": "1ln_P4XdF3nE",
        "outputId": "b71400df-cb59-4979-fb8f-3bb568d547e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-454568b9f185>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdest_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr\"mnis_train.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdest_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: \"'./datasets/data.csv'\""
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xY00OyzpzAAk"
      },
      "source": [
        "**Perfect**, Classifier also says the same!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "sentiment_analysis-Copy1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}